# Deepfake Detection System - Big Data Project

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Spark](https://img.shields.io/badge/Spark-3.3.0-orange.svg)](https://spark.apache.org/)
[![HDFS](https://img.shields.io/badge/HDFS-3.3.0-green.svg)](https://hadoop.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue.svg)](https://www.docker.com/)

Há»‡ thá»‘ng phÃ¡t hiá»‡n Deepfake sá»­ dá»¥ng **Distributed Big Data Processing** vá»›i Apache Spark, HDFS vÃ  Deep Learning.

## ğŸ“‹ Table of Contents

- [Tá»•ng Quan](#tá»•ng-quan)
- [Kiáº¿n TrÃºc Há»‡ Thá»‘ng](#kiáº¿n-trÃºc-há»‡-thá»‘ng)
- [YÃªu Cáº§u Há»‡ Thá»‘ng](#yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Äáº·t](#cÃ i-Ä‘áº·t)
- [Cáº¥u TrÃºc Project](#cáº¥u-trÃºc-project)
- [Sá»­ Dá»¥ng](#sá»­-dá»¥ng)
- [Pipeline Flow](#pipeline-flow)
- [Monitoring](#monitoring)
- [Testing](#testing)
- [Documentation](#documentation)

## ğŸ¯ Tá»•ng Quan

### Má»¥c TiÃªu
XÃ¢y dá»±ng há»‡ thá»‘ng **phÃ¢n tÃ¡n** Ä‘á»ƒ:
- âœ… Xá»­ lÃ½ **120,000 images** trÃªn HDFS
- âœ… Extract features sá»­ dá»¥ng **ResNet50** (distributed inference)
- âœ… Train ML models vá»›i **Spark MLlib**
- âœ… Äáº¡t accuracy > 85% trong viá»‡c phÃ¡t hiá»‡n deepfake

### CÃ´ng Nghá»‡ Sá»­ Dá»¥ng
- **Storage**: Hadoop HDFS (distributed file system)
- **Processing**: Apache Spark (distributed computing)
- **ML**: PyTorch (ResNet50) + Spark MLlib (RandomForest, LogisticRegression)
- **Orchestration**: Docker Compose
- **Monitoring**: Spark History Server

## ğŸ—ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEEPFAKE DETECTION SYSTEM                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data   â”‚â”€â”€â”€â”€â”€â–¶â”‚     HDFS     â”‚â”€â”€â”€â”€â”€â–¶â”‚    Spark     â”‚
â”‚  (Dataset)   â”‚      â”‚   Storage    â”‚      â”‚  Processing  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚                      â”‚
                             â”‚                      â–¼
                             â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚              â”‚   Feature    â”‚
                             â”‚              â”‚ Extraction   â”‚
                             â”‚              â”‚  (ResNet50)  â”‚
                             â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚                      â”‚
                             â”‚                      â–¼
                             â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚              â”‚  ML Training â”‚
                             â”‚              â”‚   (Spark ML) â”‚
                             â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚                      â”‚
                             â–¼                      â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   Results    â”‚â—€â”€â”€â”€â”€â”€â”‚  Evaluation  â”‚
                      â”‚  (Parquet)   â”‚      â”‚  & Metrics   â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ’» YÃªu Cáº§u Há»‡ Thá»‘ng

### Hardware
- **CPU**: 4+ cores
- **RAM**: 16GB+ (khuyáº¿n nghá»‹ 32GB)
- **Disk**: 50GB+ free space

### Software
- **Docker Desktop** (Windows/Mac) hoáº·c **Docker + Docker Compose** (Linux)
- **Python** 3.8+
- **Git**

## ğŸš€ CÃ i Äáº·t

### 1. Clone Repository
```bash
git clone https://github.com/your-username/DoAn_BigDATA.git
cd DoAn_BigDATA
```

### 2. Start Docker Services
```bash
docker-compose up -d
```

Kiá»ƒm tra containers:
```bash
docker-compose ps
```

Expected output:
```
NAME                STATUS
namenode            Up
datanode-1          Up
datanode-2          Up
spark-master        Up
spark-worker-1      Up
spark-worker-2      Up
spark-history       Up
```

### 3. Setup HDFS Directories
```bash
bash scripts/setup_hdfs.sh
```

### 4. Verify Services

**HDFS NameNode UI**: http://localhost:9870  
**Spark Master UI**: http://localhost:8080  
**Spark History Server**: http://localhost:18080

## ğŸ“ Cáº¥u TrÃºc Project

```
DoAn_TH_BIGDATA/
â”œâ”€â”€ src/                              # Source code modules
â”‚   â”œâ”€â”€ config/                       # Configuration
â”‚   â”‚   â”œâ”€â”€ hdfs_config.py
â”‚   â”‚   â”œâ”€â”€ spark_config.py
â”‚   â”‚   â””â”€â”€ model_config.py
â”‚   â”œâ”€â”€ utils/                        # Utilities
â”‚   â”‚   â”œâ”€â”€ hdfs_utils.py
â”‚   â”‚   â”œâ”€â”€ spark_utils.py
â”‚   â”‚   â”œâ”€â”€ image_utils.py
â”‚   â”‚   â””â”€â”€ logging_utils.py
â”‚   â”œâ”€â”€ 1_ingestion/                  # Data upload to HDFS
â”‚   â”‚   â”œâ”€â”€ upload_to_hdfs.py
â”‚   â”‚   â””â”€â”€ verify_upload.py
â”‚   â”œâ”€â”€ 2_preprocessing/              # Data validation
â”‚   â”‚   â”œâ”€â”€ load_data.py
â”‚   â”‚   â”œâ”€â”€ validate_images.py
â”‚   â”‚   â””â”€â”€ prepare_dataframe.py
â”‚   â”œâ”€â”€ 3_feature_extraction/         # ResNet50 features
â”‚   â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”‚   â”œâ”€â”€ feature_extractor.py
â”‚   â”‚   â””â”€â”€ extract_pipeline.py
â”‚   â”œâ”€â”€ 4_ml_training/                # Spark ML training
â”‚   â”‚   â”œâ”€â”€ prepare_vectors.py
â”‚   â”‚   â”œâ”€â”€ train_classifier.py
â”‚   â”‚   â””â”€â”€ save_model.py
â”‚   â”œâ”€â”€ 5_evaluation/                 # Model evaluation
â”‚   â”‚   â”œâ”€â”€ evaluate_model.py
â”‚   â”‚   â”œâ”€â”€ confusion_matrix.py
â”‚   â”‚   â””â”€â”€ generate_report.py
â”‚   â””â”€â”€ 6_inference/                  # Production inference
â”‚       â””â”€â”€ batch_inference.py
â”œâ”€â”€ scripts/                          # Automation scripts
â”‚   â”œâ”€â”€ setup_hdfs.sh
â”‚   â”œâ”€â”€ run_full_pipeline.sh
â”‚   â”œâ”€â”€ run_test_100_images.sh
â”‚   â””â”€â”€ check_spark_history.sh
â”œâ”€â”€ notebooks/                        # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_feature_analysis.ipynb
â”‚   â””â”€â”€ 03_model_evaluation.ipynb
â”œâ”€â”€ tests/                            # Unit tests
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_feature_extraction.py
â”‚   â””â”€â”€ test_ml_training.py
â”œâ”€â”€ data/                             # Dataset (local)
â”œâ”€â”€ models/                           # Saved models
â”œâ”€â”€ logs/                             # Application logs
â”œâ”€â”€ results/                          # Output results
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸ® Sá»­ Dá»¥ng

### Quick Test (100 Images)

**1. Prepare test dataset:**
```bash
# Copy 100 images (50 REAL + 50 FAKE)
mkdir -p Dataset_Test/train/REAL
mkdir -p Dataset_Test/train/FAKE
# Copy files...
```

**2. Run test pipeline:**
```bash
bash scripts/run_test_100_images.sh
```

Expected time: ~30 minutes

### Full Pipeline (120K Images)

**1. Upload data to HDFS:**
```bash
python src/1_ingestion/upload_to_hdfs.py \
    --local_path data/train \
    --hdfs_path /user/data/raw/train
```

**2. Extract features:**
```bash
docker exec -it spark-master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --driver-memory 4g \
    --executor-memory 4g \
    src/3_feature_extraction/extract_pipeline.py \
    --input_path /user/data/raw/train \
    --output_path /user/data/features/train
```

Expected time: 6-7 hours

**3. Train ML models:**
```bash
docker exec -it spark-master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    src/4_ml_training/train_classifier.py
```

**4. Evaluate:**
```bash
docker exec -it spark-master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    src/5_evaluation/evaluate_model.py
```

## ğŸ”„ Pipeline Flow

```
1. INGESTION
   â””â”€> Upload local images to HDFS
       Input:  local Dataset/train/*.jpg
       Output: HDFS /user/data/raw/train/

2. PREPROCESSING
   â””â”€> Load with Spark binaryFile, validate, label
       Input:  HDFS /user/data/raw/train/
       Output: DataFrame[path, content, label]

3. FEATURE EXTRACTION (Distributed)
   â””â”€> ResNet50 inference on Spark Workers
       Input:  DataFrame[content]
       Output: HDFS /user/data/features/ (Parquet)

4. ML TRAINING
   â””â”€> Train RandomForest + LogisticRegression
       Input:  HDFS /user/data/features/
       Output: HDFS /user/models/

5. EVALUATION
   â””â”€> Calculate metrics, generate report
       Input:  Model + test data
       Output: HDFS /user/data/results/
```

## ğŸ“Š Monitoring

### Spark History Server
```bash
# Access at http://localhost:18080

# Or use script:
bash scripts/check_spark_history.sh
```

**Screenshots cáº§n capture:**
- Job timeline
- Stage details (parallel tasks)
- Executor metrics
- DAG visualization

### HDFS NameNode UI
```bash
# Access at http://localhost:9870

# Check files:
docker exec namenode hdfs dfs -ls -R /user/data
```

## ğŸ§ª Testing

### Run Unit Tests
```bash
# All tests
python -m pytest tests/

# Specific module
python -m pytest tests/test_config.py

# With coverage
python -m pytest tests/ --cov=src
```

### Test Configuration
```bash
python tests/test_config.py
```

## ğŸ“š Documentation

Xem chi tiáº¿t táº¡i:
- **[ARCHITECTURE_ANALYSIS.md](ARCHITECTURE_ANALYSIS.md)** - Kiáº¿n trÃºc chi tiáº¿t
- **[API Documentation](docs/API.md)** - API reference
- **[Troubleshooting Guide](docs/TROUBLESHOOTING.md)** - Xá»­ lÃ½ lá»—i

## ğŸ¯ Expected Results

### Test (100 images)
- **Accuracy**: ~70-80%
- **Processing time**: ~30 minutes
- **Feature dimension**: 2048 (ResNet50)

### Full (120K images)
- **Accuracy**: > 85%
- **Processing time**: ~6-7 hours
- **Models**: RandomForest + LogisticRegression

## ğŸ› Troubleshooting

### Common Issues

**1. Docker containers not starting:**
```bash
docker-compose down
docker-compose up -d
docker-compose logs -f
```

**2. HDFS connection timeout:**
```bash
# Check namenode
docker exec namenode hdfs dfsadmin -report

# Restart HDFS
docker restart namenode datanode-1 datanode-2
```

**3. Spark job fails:**
```bash
# Check Spark logs
docker logs spark-master
docker logs spark-worker-1

# Check Spark History
open http://localhost:18080
```

## ğŸ‘¥ Contributors

- **Team**: BigData Team
- **Project**: Deepfake Detection System
- **Course**: Big Data Processing

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- **Apache Spark** - Distributed computing
- **Hadoop HDFS** - Distributed storage
- **PyTorch** - Deep learning framework
- **Docker** - Containerization

---

**Last Updated**: December 16, 2025
