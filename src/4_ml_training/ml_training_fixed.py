"""
Step 3: FIXED BATCH ML Training - Load FULL 100K Data
Fixed version that properly loads all training samples
"""

from pyspark.sql import SparkSession
from pyspark.ml.classification import LogisticRegression, RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.sql.functions import col
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.sql.functions import udf
import time

# ===== BÆ¯á»šC 1: Khá»Ÿi táº¡o Spark Session =====
print("ğŸš€ Initializing Spark Session for FIXED BATCH ML Training...")
spark = SparkSession.builder \
    .appName("DeepfakeDetection-FixedBatchTraining") \
    .config("spark.executor.memory", "2g") \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.cores", "2") \
    .config("spark.sql.shuffle.partitions", "100") \
    .config("spark.default.parallelism", "100") \
    .config("spark.eventLog.enabled", "true") \
    .config("spark.eventLog.dir", "hdfs://namenode:8020/spark-logs") \
    .getOrCreate()

print(f"âœ… Spark version: {spark.version}")
print(f"âœ… Spark master: {spark.sparkContext.master}")

# ===== BÆ¯á»šC 2: Load ALL Features - FIXED APPROACH =====
print("\n" + "="*70)
print("ğŸ“‚ LOADING FEATURES WITH FIXED BATCH STRATEGY")
print("="*70)

# Define paths
train_real_batches = [f"hdfs://namenode:8020/user/data/features/train/REAL/batch_{i}" for i in range(1, 6)]
train_fake_batches = [f"hdfs://namenode:8020/user/data/features/train/FAKE/batch_{i}" for i in range(1, 6)]
test_real_path = "hdfs://namenode:8020/user/data/features/test/REAL"
test_fake_path = "hdfs://namenode:8020/user/data/features/test/FAKE"

print(f"\nğŸ“¦ Training Batch Configuration:")
print(f"   - REAL batches: {len(train_real_batches)}")
print(f"   - FAKE batches: {len(train_fake_batches)}")
print(f"   - Total batches: {len(train_real_batches) + len(train_fake_batches)}")

# ===== LOAD TRAINING DATA - USE unionAll INSTEAD =====
print("\nğŸ”„ Loading REAL training batches...")
train_real_df = None
for i, path in enumerate(train_real_batches, 1):
    print(f"  [{i}/{len(train_real_batches)}] Loading REAL batch_{i}...")
    df = spark.read.parquet(path)
    count = df.count()
    print(f"      â†’ Loaded {count:,} samples")
    if train_real_df is None:
        train_real_df = df
    else:
        train_real_df = train_real_df.union(df)

real_total = train_real_df.count()
print(f"âœ… Total REAL samples: {real_total:,}")

print("\nğŸ”„ Loading FAKE training batches...")
train_fake_df = None
for i, path in enumerate(train_fake_batches, 1):
    print(f"  [{i}/{len(train_fake_batches)}] Loading FAKE batch_{i}...")
    df = spark.read.parquet(path)
    count = df.count()
    print(f"      â†’ Loaded {count:,} samples")
    if train_fake_df is None:
        train_fake_df = df
    else:
        train_fake_df = train_fake_df.union(df)

fake_total = train_fake_df.count()
print(f"âœ… Total FAKE samples: {fake_total:,}")

# Combine REAL + FAKE
print("\nğŸ”— Combining REAL + FAKE training data...")
train_df = train_real_df.union(train_fake_df)
train_count = train_df.count()
print(f"âœ… TOTAL TRAINING SAMPLES: {train_count:,}")

# Load test data
print("\nğŸ“‚ Loading test data...")
print("  - Loading test REAL...")
test_real_df = spark.read.parquet(test_real_path)
real_test_count = test_real_df.count()
print(f"      â†’ {real_test_count:,} samples")

print("  - Loading test FAKE...")
test_fake_df = spark.read.parquet(test_fake_path)
fake_test_count = test_fake_df.count()
print(f"      â†’ {fake_test_count:,} samples")

test_df = test_real_df.union(test_fake_df)
test_count = test_df.count()
print(f"âœ… TOTAL TEST SAMPLES: {test_count:,}")

# Show label distribution
print("\nğŸ“Š Training Label Distribution:")
train_df.groupBy("label").count().orderBy("label").show()

print("\nğŸ“Š Test Label Distribution:")
test_df.groupBy("label").count().orderBy("label").show()

# ===== BÆ¯á»šC 3: Convert Features to Vector =====
print("\n" + "="*70)
print("ğŸ”„ CONVERTING FEATURES TO VECTOR TYPE")
print("="*70)

def list_to_vector(features_list):
    """Convert list of floats to DenseVector"""
    return Vectors.dense(features_list)

# Create UDF
list_to_vector_udf = udf(list_to_vector, VectorUDT())

# Apply transformation
print("\n  - Converting training features...")
train_ml = train_df.select(
    col("path"),
    list_to_vector_udf(col("features")).alias("features"),
    col("label")
)

print("  - Converting test features...")
test_ml = test_df.select(
    col("path"),
    list_to_vector_udf(col("features")).alias("features"),
    col("label")
)

print("âœ… Features converted to Vector type")

# Repartition and cache
print("\nâš¡ Optimizing data distribution...")
train_ml = train_ml.repartition(100).cache()
test_ml = test_ml.repartition(50).cache()

# Force cache with count
print("ğŸ’¾ Caching training data...")
cached_train_count = train_ml.count()
print(f"  âœ… {cached_train_count:,} training samples cached")

print("ğŸ’¾ Caching test data...")
cached_test_count = test_ml.count()
print(f"  âœ… {cached_test_count:,} test samples cached")

print("\nâœ… Data preparation completed!")
train_ml.show(5, truncate=False)

# ===== BÆ¯á»šC 4: Train Logistic Regression =====
print("\n" + "="*70)
print("ğŸ¤– TRAINING LOGISTIC REGRESSION MODEL")
print("="*70)

lr_start = time.time()

lr = LogisticRegression(
    featuresCol="features",
    labelCol="label",
    maxIter=100,
    regParam=0.01,
    elasticNetParam=0.8,
    aggregationDepth=5
)

print(f"\nğŸ“‹ Model Configuration:")
print(f"   - Training Samples: {cached_train_count:,}")
print(f"   - Max Iterations: {lr.getMaxIter()}")
print(f"   - Regularization: {lr.getRegParam()}")
print(f"   - ElasticNet Param: {lr.getElasticNetParam()}")

print(f"\nğŸ‹ï¸ Fitting Logistic Regression on {cached_train_count:,} samples...")
lr_model = lr.fit(train_ml)

lr_duration = time.time() - lr_start
print(f"âœ… Logistic Regression trained in {lr_duration:.2f} seconds")

# Predict
print("\nğŸ”® Making predictions on test set...")
lr_predictions = lr_model.transform(test_ml)

# Save predictions
lr_output_path = "hdfs://namenode:8020/user/data/results/lr_predictions_fixed"
print(f"ğŸ’¾ Saving predictions to {lr_output_path}")
lr_predictions.select("path", "label", "prediction", "probability").write.mode("overwrite").parquet(lr_output_path)
print("âœ… LR Predictions saved!")

# ===== BÆ¯á»šC 5: Train Random Forest =====
print("\n" + "="*70)
print("ğŸŒ² TRAINING RANDOM FOREST MODEL")
print("="*70)

rf_start = time.time()

rf = RandomForestClassifier(
    featuresCol="features",
    labelCol="label",
    numTrees=100,
    maxDepth=10,
    maxBins=32,
    seed=42,
    subsamplingRate=0.8
)

print(f"\nğŸ“‹ Model Configuration:")
print(f"   - Training Samples: {cached_train_count:,}")
print(f"   - Number of Trees: {rf.getNumTrees()}")
print(f"   - Max Depth: {rf.getMaxDepth()}")
print(f"   - Max Bins: {rf.getMaxBins()}")
print(f"   - Subsampling Rate: {rf.getSubsamplingRate()}")

print(f"\nğŸ‹ï¸ Fitting Random Forest on {cached_train_count:,} samples...")
rf_model = rf.fit(train_ml)

rf_duration = time.time() - rf_start
print(f"âœ… Random Forest trained in {rf_duration:.2f} seconds")

# Predict
print("\nğŸ”® Making predictions on test set...")
rf_predictions = rf_model.transform(test_ml)

# Save predictions
rf_output_path = "hdfs://namenode:8020/user/data/results/rf_predictions_fixed"
print(f"ğŸ’¾ Saving predictions to {rf_output_path}")
rf_predictions.select("path", "label", "prediction", "probability").write.mode("overwrite").parquet(rf_output_path)
print("âœ… RF Predictions saved!")

# ===== BÆ¯á»šC 6: Model Evaluation =====
print("\n" + "="*70)
print("ğŸ“Š MODEL EVALUATION")
print("="*70)

# Evaluators
binary_evaluator = BinaryClassificationEvaluator(labelCol="label", metricName="areaUnderROC")
multiclass_evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction")

# Logistic Regression Metrics
print("\nğŸ”¹ LOGISTIC REGRESSION RESULTS:")
print("-" * 70)
lr_auc = binary_evaluator.evaluate(lr_predictions)
lr_accuracy = multiclass_evaluator.evaluate(lr_predictions, {multiclass_evaluator.metricName: "accuracy"})
lr_precision = multiclass_evaluator.evaluate(lr_predictions, {multiclass_evaluator.metricName: "weightedPrecision"})
lr_recall = multiclass_evaluator.evaluate(lr_predictions, {multiclass_evaluator.metricName: "weightedRecall"})
lr_f1 = multiclass_evaluator.evaluate(lr_predictions, {multiclass_evaluator.metricName: "f1"})

print(f"  ğŸ“ˆ AUC-ROC:        {lr_auc:.4f}")
print(f"  ğŸ¯ Accuracy:       {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)")
print(f"  ğŸ¯ Precision:      {lr_precision:.4f}")
print(f"  ğŸ¯ Recall:         {lr_recall:.4f}")
print(f"  ğŸ¯ F1-Score:       {lr_f1:.4f}")
print(f"  â±ï¸  Training Time:  {lr_duration:.2f} seconds")

# Random Forest Metrics
print("\nğŸ”¹ RANDOM FOREST RESULTS:")
print("-" * 70)
rf_auc = binary_evaluator.evaluate(rf_predictions)
rf_accuracy = multiclass_evaluator.evaluate(rf_predictions, {multiclass_evaluator.metricName: "accuracy"})
rf_precision = multiclass_evaluator.evaluate(rf_predictions, {multiclass_evaluator.metricName: "weightedPrecision"})
rf_recall = multiclass_evaluator.evaluate(rf_predictions, {multiclass_evaluator.metricName: "weightedRecall"})
rf_f1 = multiclass_evaluator.evaluate(rf_predictions, {multiclass_evaluator.metricName: "f1"})

print(f"  ğŸ“ˆ AUC-ROC:        {rf_auc:.4f}")
print(f"  ğŸ¯ Accuracy:       {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)")
print(f"  ğŸ¯ Precision:      {rf_precision:.4f}")
print(f"  ğŸ¯ Recall:         {rf_recall:.4f}")
print(f"  ğŸ¯ F1-Score:       {rf_f1:.4f}")
print(f"  â±ï¸  Training Time:  {rf_duration:.2f} seconds")

# Confusion Matrix
print("\nğŸ“ˆ CONFUSION MATRIX - Logistic Regression:")
print("-" * 70)
lr_cm = lr_predictions.groupBy("label", "prediction").count().orderBy("label", "prediction")
lr_cm.show()

print("\nğŸ“ˆ CONFUSION MATRIX - Random Forest:")
print("-" * 70)
rf_cm = rf_predictions.groupBy("label", "prediction").count().orderBy("label", "prediction")
rf_cm.show()

# ===== BÆ¯á»šC 7: Save Models =====
print("\n" + "="*70)
print("ğŸ’¾ SAVING TRAINED MODELS")
print("="*70)

lr_model_path = "hdfs://namenode:8020/user/data/models/logistic_regression_fixed"
rf_model_path = "hdfs://namenode:8020/user/data/models/random_forest_fixed"

print(f"\nğŸ“¦ Saving Logistic Regression model...")
print(f"   â†’ {lr_model_path}")
lr_model.write().overwrite().save(lr_model_path)
print("   âœ… LR Model saved!")

print(f"\nğŸ“¦ Saving Random Forest model...")
print(f"   â†’ {rf_model_path}")
rf_model.write().overwrite().save(rf_model_path)
print("   âœ… RF Model saved!")

# ===== BÆ¯á»šC 8: Save Metrics Report =====
print("\n" + "="*70)
print("ğŸ“ GENERATING METRICS REPORT")
print("="*70)

metrics_data = [
    ("Logistic Regression", lr_accuracy, lr_precision, lr_recall, lr_f1, lr_auc, lr_duration, cached_train_count),
    ("Random Forest", rf_accuracy, rf_precision, rf_recall, rf_f1, rf_auc, rf_duration, cached_train_count)
]

from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType

metrics_schema = StructType([
    StructField("model", StringType(), False),
    StructField("accuracy", DoubleType(), False),
    StructField("precision", DoubleType(), False),
    StructField("recall", DoubleType(), False),
    StructField("f1_score", DoubleType(), False),
    StructField("auc_roc", DoubleType(), False),
    StructField("training_time_seconds", DoubleType(), False),
    StructField("training_samples", LongType(), False)
])

metrics_df = spark.createDataFrame(metrics_data, schema=metrics_schema)
metrics_path = "hdfs://namenode:8020/user/data/results/metrics_summary_fixed"

print(f"\nğŸ’¾ Saving metrics to {metrics_path}")
metrics_df.write.mode("overwrite").parquet(metrics_path)

print("\nâœ… Metrics saved!")
print("\nğŸ“Š FINAL METRICS SUMMARY:")
print("-" * 70)
metrics_df.show(truncate=False)

# ===== HOÃ€N Táº¤T =====
print("\n" + "="*70)
print("ğŸ‰ FIXED BATCH ML TRAINING COMPLETED!")
print("="*70)

print(f"\nğŸ“Š TRAINING SUMMARY:")
print(f"   âœ“ Training Samples: {cached_train_count:,}")
print(f"   âœ“ Test Samples:     {cached_test_count:,}")
print(f"   âœ“ Total Runtime:    {lr_duration + rf_duration:.2f} seconds")

print(f"\nğŸ“¦ SAVED ARTIFACTS:")
print(f"   âœ“ LR Model:         {lr_model_path}")
print(f"   âœ“ RF Model:         {rf_model_path}")
print(f"   âœ“ LR Predictions:   {lr_output_path}")
print(f"   âœ“ RF Predictions:   {rf_output_path}")
print(f"   âœ“ Metrics Summary:  {metrics_path}")

print(f"\nğŸ¯ NEXT STEPS:")
print(f"   1. Check Spark History Server: http://localhost:18080")
print(f"   2. Review confusion matrices and accuracy metrics")
print(f"   3. Compare with previous run (64 samples vs {cached_train_count:,} samples)")
print(f"   4. Take screenshots for project report")

print("\nâœ¨ Thank you for using FIXED Distributed ML Training Pipeline!")
print("="*70)

spark.stop()
