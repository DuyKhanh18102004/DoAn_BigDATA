#!/usr/bin/env python3
"""
ML Training with Batched Data Loading
Train models on 100K samples by loading 10K at a time to avoid memory issues
"""

from pyspark.sql import SparkSession
from pyspark.ml.classification import LogisticRegression, RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.sql.functions import col
import time
import gc

print("="*80)
print("ğŸ¤– ML TRAINING - BATCHED APPROACH")
print("Training on 100K samples by loading 10K batches")
print("="*80)

# Initialize Spark
spark = SparkSession.builder \
    .appName("ML_Training_Batched") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "2") \
    .config("spark.sql.shuffle.partitions", "50") \
    .getOrCreate()

pipeline_start = time.time()

# ============================================================================
# STEP 1: LOAD TRAINING DATA IN BATCHES
# ============================================================================

print("\n" + "ğŸ“š"*40)
print("STEP 1: LOADING TRAINING DATA (100K samples in batches)")
print("ğŸ“š"*40)

train_batches = []

# Load TRAIN/REAL batches (5 batches Ã— ~10K = 50K)
print("\nğŸŸ¢ Loading TRAIN/REAL batches...")
for batch_num in range(1, 6):
    path = f"hdfs://namenode:8020/user/data/features/train/REAL/batch_{batch_num}"
    print(f"  Loading batch {batch_num}...")
    df_batch = spark.read.parquet(path)
    count = df_batch.count()
    print(f"  âœ… Batch {batch_num}: {count:,} samples")
    train_batches.append(df_batch)
    time.sleep(2)

# Load TRAIN/FAKE batches (5 batches Ã— ~10K = 50K)
print("\nğŸ”´ Loading TRAIN/FAKE batches...")
for batch_num in range(1, 6):
    path = f"hdfs://namenode:8020/user/data/features/train/FAKE/batch_{batch_num}"
    print(f"  Loading batch {batch_num}...")
    df_batch = spark.read.parquet(path)
    count = df_batch.count()
    print(f"  âœ… Batch {batch_num}: {count:,} samples")
    train_batches.append(df_batch)
    time.sleep(2)

# Combine all training batches
print("\nğŸ”— Combining all training batches...")
df_train = train_batches[0]
for batch in train_batches[1:]:
    df_train = df_train.union(batch)

# Cache and count
df_train = df_train.repartition(50).cache()
train_count = df_train.count()
print(f"âœ… Total training samples: {train_count:,}")

# Show distribution
print("\nğŸ“Š Training data distribution:")
train_dist = df_train.groupBy("label").count().collect()
for row in train_dist:
    label_name = "REAL" if row['label'] == 1 else "FAKE"
    print(f"  {label_name}: {row['count']:,} samples")

# Memory cleanup
for batch in train_batches:
    batch.unpersist()
train_batches.clear()
gc.collect()
time.sleep(5)

# ============================================================================
# STEP 2: LOAD TEST DATA
# ============================================================================

print("\n" + "ğŸ§ª"*40)
print("STEP 2: LOADING TEST DATA (20K samples)")
print("ğŸ§ª"*40)

print("\nğŸŸ¢ Loading TEST/REAL...")
df_test_real = spark.read.parquet("hdfs://namenode:8020/user/data/features/test/REAL/batch_1")
real_count = df_test_real.count()
print(f"âœ… TEST/REAL: {real_count:,} samples")

print("\nğŸ”´ Loading TEST/FAKE...")
df_test_fake = spark.read.parquet("hdfs://namenode:8020/user/data/features/test/FAKE/batch_1")
fake_count = df_test_fake.count()
print(f"âœ… TEST/FAKE: {fake_count:,} samples")

# Combine test data
df_test = df_test_real.union(df_test_fake)
df_test = df_test.repartition(20).cache()
test_count = df_test.count()
print(f"\nâœ… Total test samples: {test_count:,}")

# Show distribution
print("\nğŸ“Š Test data distribution:")
test_dist = df_test.groupBy("label").count().collect()
for row in test_dist:
    label_name = "REAL" if row['label'] == 1 else "FAKE"
    print(f"  {label_name}: {row['count']:,} samples")

# ============================================================================
# STEP 3: TRAIN LOGISTIC REGRESSION
# ============================================================================

print("\n" + "ğŸ¯"*40)
print("STEP 3: TRAINING LOGISTIC REGRESSION")
print("ğŸ¯"*40)

lr_start = time.time()

print("\nğŸ”§ Creating Logistic Regression model...")
lr = LogisticRegression(
    featuresCol="features",
    labelCol="label",
    maxIter=20,
    regParam=0.01,
    elasticNetParam=0.0
)

print("ğŸš€ Training Logistic Regression...")
lr_model = lr.fit(df_train)
lr_elapsed = time.time() - lr_start
print(f"âœ… Training completed in {lr_elapsed:.2f}s")

print("\nğŸ“Š Evaluating Logistic Regression...")
lr_predictions = lr_model.transform(df_test)

# Binary metrics
evaluator_auc = BinaryClassificationEvaluator(labelCol="label", metricName="areaUnderROC")
lr_auc = evaluator_auc.evaluate(lr_predictions)

# Multiclass metrics
evaluator_acc = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
evaluator_prec = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedPrecision")
evaluator_rec = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedRecall")
evaluator_f1 = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")

lr_acc = evaluator_acc.evaluate(lr_predictions)
lr_prec = evaluator_prec.evaluate(lr_predictions)
lr_rec = evaluator_rec.evaluate(lr_predictions)
lr_f1 = evaluator_f1.evaluate(lr_predictions)

print("\nğŸ“ˆ LOGISTIC REGRESSION RESULTS:")
print(f"  â­ Accuracy:  {lr_acc*100:.2f}%")
print(f"  ğŸ“Š AUC:       {lr_auc:.4f}")
print(f"  ğŸ¯ Precision: {lr_prec:.4f}")
print(f"  ğŸ” Recall:    {lr_rec:.4f}")
print(f"  âš–ï¸  F1-Score:  {lr_f1:.4f}")

# Save model
lr_model_path = "hdfs://namenode:8020/user/data/models/logistic_regression_batched"
print(f"\nğŸ’¾ Saving model to {lr_model_path}")
lr_model.write().overwrite().save(lr_model_path)
print("âœ… Model saved successfully")

# Memory cleanup
lr_predictions.unpersist()
del lr_predictions
gc.collect()
time.sleep(5)

# ============================================================================
# STEP 4: TRAIN RANDOM FOREST
# ============================================================================

print("\n" + "ğŸŒ²"*40)
print("STEP 4: TRAINING RANDOM FOREST")
print("ğŸŒ²"*40)

rf_start = time.time()

print("\nğŸ”§ Creating Random Forest model...")
rf = RandomForestClassifier(
    featuresCol="features",
    labelCol="label",
    numTrees=50,
    maxDepth=10,
    maxBins=32,
    seed=42
)

print("ğŸš€ Training Random Forest (this may take several minutes)...")
rf_model = rf.fit(df_train)
rf_elapsed = time.time() - rf_start
print(f"âœ… Training completed in {rf_elapsed/60:.2f} minutes")

print("\nğŸ“Š Evaluating Random Forest...")
rf_predictions = rf_model.transform(df_test)

# Binary metrics
rf_auc = evaluator_auc.evaluate(rf_predictions)

# Multiclass metrics
rf_acc = evaluator_acc.evaluate(rf_predictions)
rf_prec = evaluator_prec.evaluate(rf_predictions)
rf_rec = evaluator_rec.evaluate(rf_predictions)
rf_f1 = evaluator_f1.evaluate(rf_predictions)

print("\nğŸ“ˆ RANDOM FOREST RESULTS:")
print(f"  â­ Accuracy:  {rf_acc*100:.2f}%")
print(f"  ğŸ“Š AUC:       {rf_auc:.4f}")
print(f"  ğŸ¯ Precision: {rf_prec:.4f}")
print(f"  ğŸ” Recall:    {rf_rec:.4f}")
print(f"  âš–ï¸  F1-Score:  {rf_f1:.4f}")

# Feature importance
print("\nğŸ¯ Top 10 Feature Importances:")
feature_importances = rf_model.featureImportances.toArray()
top_indices = feature_importances.argsort()[-10:][::-1]
for idx in top_indices:
    print(f"  Feature {idx}: {feature_importances[idx]:.6f}")

# Save model
rf_model_path = "hdfs://namenode:8020/user/data/models/random_forest_batched"
print(f"\nğŸ’¾ Saving model to {rf_model_path}")
rf_model.write().overwrite().save(rf_model_path)
print("âœ… Model saved successfully")

# ============================================================================
# FINAL SUMMARY
# ============================================================================

pipeline_elapsed = time.time() - pipeline_start

print("\n" + "="*80)
print("ğŸ“Š FINAL SUMMARY")
print("="*80)

print(f"\nğŸ“š Training Data: {train_count:,} samples")
print(f"ğŸ§ª Test Data: {test_count:,} samples")
print(f"â±ï¸  Total Pipeline Time: {pipeline_elapsed/60:.2f} minutes")

print("\n" + "ğŸ†"*40)
print("MODEL COMPARISON")
print("ğŸ†"*40)

print("\nğŸ“ˆ Logistic Regression:")
print(f"  Accuracy:  {lr_acc*100:.2f}%")
print(f"  F1-Score:  {lr_f1:.4f}")
print(f"  Time:      {lr_elapsed:.2f}s")

print("\nğŸ“ˆ Random Forest:")
print(f"  Accuracy:  {rf_acc*100:.2f}%")
print(f"  F1-Score:  {rf_f1:.4f}")
print(f"  Time:      {rf_elapsed/60:.2f} min")

# Determine winner
if rf_acc > lr_acc:
    winner = "Random Forest"
    winner_acc = rf_acc
else:
    winner = "Logistic Regression"
    winner_acc = lr_acc

print(f"\nğŸ¥‡ BEST MODEL: {winner} ({winner_acc*100:.2f}% accuracy)")

# Check if target achieved
target_acc = 0.85
if winner_acc >= target_acc:
    print("\n" + "ğŸ‰"*40)
    print(f"âœ… TARGET ACHIEVED! Accuracy {winner_acc*100:.2f}% >= {target_acc*100:.0f}%")
    print("ğŸ‰"*40)
else:
    print("\n" + "âš ï¸"*40)
    print(f"âš ï¸  Target not reached. Current: {winner_acc*100:.2f}%, Target: {target_acc*100:.0f}%")
    print("âš ï¸"*40)

print("\n" + "="*80)
print("âœ… ML TRAINING PIPELINE COMPLETED")
print("="*80)

spark.stop()
