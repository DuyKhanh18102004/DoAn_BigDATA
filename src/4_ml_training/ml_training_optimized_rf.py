#!/usr/bin/env python3
"""
ML Training - OPTIMIZED Logistic Regression with BATCH PROCESSING
Train on 100K images using 10K batch processing to avoid OOM
- Xá»­ lÃ½ 10K hÃ¬nh má»—i batch, release memory sau má»—i batch
- Sá»­ dá»¥ng Logistic Regression (nháº¹ hÆ¡n Random Forest)
- Memory-safe: Load â†’ Train â†’ Predict â†’ Save â†’ Release
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, lit, avg as spark_avg, when,
    hash as spark_hash, concat_ws, udf
)
from pyspark.sql.types import DoubleType, StringType
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import StandardScaler
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator
from pyspark.ml.functions import vector_to_array
from pyspark.ml import Pipeline
import time
import gc

print("="*80)
print("ğŸ“ˆ ML TRAINING - LOGISTIC REGRESSION (10K BATCH PROCESSING)")
print("="*80)

# Initialize Spark with ULTRA MEMORY-SAFE config
spark = SparkSession.builder \
    .appName("ML_Training_LR_BatchProcessing") \
    .config("spark.driver.memory", "1200m") \
    .config("spark.executor.memory", "1200m") \
    .config("spark.executor.cores", "1") \
    .config("spark.default.parallelism", "10") \
    .config("spark.sql.shuffle.partitions", "10") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.memory.fraction", "0.5") \
    .config("spark.memory.storageFraction", "0.2") \
    .config("spark.cleaner.periodicGC.interval", "30s") \
    .config("spark.rdd.compress", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()

# Set log level Ä‘á»ƒ giáº£m noise
spark.sparkContext.setLogLevel("WARN")

pipeline_start = time.time()

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def create_image_id(df, source_type):
    """
    Táº¡o image_id duy nháº¥t tá»« features hash + source info.
    Äáº£m báº£o ID nháº¥t quÃ¡n khi join giá»¯a cÃ¡c model predictions.
    """
    # Táº¡o hash tá»« features vector Ä‘á»ƒ cÃ³ ID duy nháº¥t cho má»—i áº£nh
    # Káº¿t há»£p vá»›i source_type Ä‘á»ƒ trÃ¡nh collision giá»¯a REAL/FAKE
    return df.withColumn(
        "image_id",
        concat_ws("_", 
            lit(source_type),
            spark_hash(col("features").cast("string"))
        ).cast(StringType())
    )

def get_probability_at_index(prob_col, index):
    """Extract probability at specific index from probability vector."""
    return vector_to_array(prob_col)[index]

# ============================================================================
# CONFIGURATION
# ============================================================================

HDFS_BASE = "hdfs://namenode:8020/user/data"
FEATURES_BASE = f"{HDFS_BASE}/features_tf"  # NEW: TensorFlow MobileNetV2 features
NUM_BATCHES = 50  # 50 batches per category (50K REAL + 50K FAKE = 100K train)
VALIDATION_RATIO = 0.15  # 15% for validation
SEED = 42
BATCH_SIZE = 10000  # Xá»­ lÃ½ 10K hÃ¬nh má»—i batch

# Logistic Regression Hyperparameters - Tá»‘i Æ°u cho deep features
LR_PARAMS = {
    "maxIter": 50,            # Giáº£m Ä‘á»ƒ nhanh hÆ¡n
    "regParam": 0.01,         # L2 regularization
    "elasticNetParam": 0.0,   # 0 = L2
    "tol": 1e-5,              # TÄƒng tolerance Ä‘á»ƒ converge nhanh hÆ¡n
    "fitIntercept": True,
    "standardization": True,
    "threshold": 0.5
}

def force_cleanup():
    """Aggressive memory cleanup"""
    spark.catalog.clearCache()
    for _ in range(5):
        gc.collect()
    time.sleep(2)

# ============================================================================
# STEP 1: Load vÃ  Prepare Training Data tá»« 5 Batches
# ============================================================================

print("\n" + "="*80)
print("ğŸ“‚ STEP 1: Loading Training Data (5 Mixed Batches)")
print("="*80)

all_train_data = []
total_train_samples = 0

for batch_id in range(1, NUM_BATCHES + 1):
    print(f"\nğŸ“¦ Loading Batch {batch_id}/{NUM_BATCHES}...")
    
    # Load REAL
    real_path = f"{FEATURES_BASE}/train/REAL/batch_{batch_id}"
    df_real = spark.read.parquet(real_path)
    df_real = create_image_id(df_real, f"REAL_batch{batch_id}")
    real_count = df_real.count()
    
    # Load FAKE  
    fake_path = f"{FEATURES_BASE}/train/FAKE/batch_{batch_id}"
    df_fake = spark.read.parquet(fake_path)
    df_fake = create_image_id(df_fake, f"FAKE_batch{batch_id}")
    fake_count = df_fake.count()
    
    # Union vÃ  thÃªm batch_id
    df_batch = df_real.union(df_fake).withColumn("batch_id", lit(batch_id))
    all_train_data.append(df_batch)
    
    batch_count = real_count + fake_count
    total_train_samples += batch_count
    print(f"   âœ… REAL: {real_count:,} | FAKE: {fake_count:,} | Total: {batch_count:,}")

# Union táº¥t cáº£ batches
print(f"\nğŸ”— Combining all {NUM_BATCHES} batches...")
df_train_full = all_train_data[0]
for df in all_train_data[1:]:
    df_train_full = df_train_full.union(df)

# Repartition nhá» hÆ¡n Ä‘á»ƒ tá»‘i Æ°u memory
df_train_full = df_train_full.repartition(20, "image_id").cache()
actual_count = df_train_full.count()

print(f"âœ… Total training data: {actual_count:,} samples")
print(f"   - Expected: {total_train_samples:,}")

# ============================================================================
# STEP 2: Split Train/Validation (80/20)
# ============================================================================

print("\n" + "="*80)
print("ğŸ“Š STEP 2: Creating Train/Validation Split")
print("="*80)

# Split vá»›i stratified sampling Ä‘á»ƒ giá»¯ balance giá»¯a classes
df_train, df_val = df_train_full.randomSplit([1-VALIDATION_RATIO, VALIDATION_RATIO], seed=SEED)
df_train = df_train.cache()
df_val = df_val.cache()

train_count = df_train.count()
val_count = df_val.count()

# Kiá»ƒm tra class distribution
train_label_dist = df_train.groupBy("label").count().collect()
val_label_dist = df_val.groupBy("label").count().collect()

print(f"âœ… Training set: {train_count:,} samples")
for row in train_label_dist:
    label_name = "REAL" if row["label"] == 1 else "FAKE"
    print(f"   - {label_name} (label={row['label']}): {row['count']:,}")

print(f"\nâœ… Validation set: {val_count:,} samples")
for row in val_label_dist:
    label_name = "REAL" if row["label"] == 1 else "FAKE"
    print(f"   - {label_name} (label={row['label']}): {row['count']:,}")

# Unpersist full data Ä‘á»ƒ giáº£i phÃ³ng memory
df_train_full.unpersist()

# ============================================================================
# STEP 3: Train Random Forest Models vá»›i Cross-Validation trÃªn Batches
# ============================================================================

print("\n" + "="*80)
print("ğŸ“ˆ STEP 3: Training Logistic Regression Models")
print("="*80)
print("\nğŸ”§ HYPERPARAMETERS:")
for key, value in LR_PARAMS.items():
    print(f"   - {key}: {value}")

# Láº¥y unique batches Ä‘á»ƒ train riÃªng biá»‡t (ensemble approach)
batch_ids = [row["batch_id"] for row in df_train.select("batch_id").distinct().collect()]
batch_ids.sort()

print(f"\nğŸ“¦ Training {len(batch_ids)} models (one per batch)...")

model_predictions = []
individual_accuracies = []

for batch_id in batch_ids:
    print(f"\n{'='*80}")
    print(f"ğŸ“¦ Model {batch_id}/{len(batch_ids)}: Training on Batch {batch_id}")
    print(f"{'='*80}")
    
    batch_start = time.time()
    
    # Filter data cho batch nÃ y
    df_batch_train = df_train.filter(col("batch_id") == batch_id).cache()
    batch_count = df_batch_train.count()
    print(f"   ğŸ“Š Training samples: {batch_count:,}")
    
    # Step 1: Feature Standardization (QUAN TRá»ŒNG: tÄƒng accuracy cho LR)
    print("   ğŸ“ Standardizing features...")
    scaler = StandardScaler(
        inputCol="features",
        outputCol="scaledFeatures",
        withStd=True,
        withMean=True  # Center features around 0
    )
    scaler_model = scaler.fit(df_batch_train)
    df_batch_scaled = scaler_model.transform(df_batch_train)
    
    # Step 2: Train Logistic Regression (nháº¹ hÆ¡n RF ráº¥t nhiá»u!)
    print("   ğŸ“ˆ Training Logistic Regression...")
    lr = LogisticRegression(
        featuresCol="scaledFeatures",  # DÃ¹ng scaled features
        labelCol="label",
        predictionCol="prediction",
        probabilityCol="probability",
        **LR_PARAMS
    )
    
    lr_model = lr.fit(df_batch_scaled)
    print("   âœ… Training completed!")
    
    # Predict trÃªn VALIDATION set - cáº§n scale validation data vá»›i cÃ¹ng scaler
    print("   ğŸ”® Predicting on validation set...")
    df_val_scaled = scaler_model.transform(df_val)
    predictions = lr_model.transform(df_val_scaled)
    
    # LÆ°u predictions vá»›i image_id thá»±c (QUAN TRá»ŒNG: khÃ´ng dÃ¹ng monotonically_increasing_id)
    # Extract probability cho class 1 (REAL) Ä‘á»ƒ ensemble averaging
    predictions = predictions.select(
        col("image_id"),
        col("label"),
        col("prediction").alias(f"pred_{batch_id}"),
        vector_to_array(col("probability"))[1].alias(f"prob_{batch_id}")
    )
    
    # Cache Ä‘á»ƒ dÃ¹ng cho ensemble
    predictions = predictions.cache()
    pred_count = predictions.count()
    
    # Evaluate single model accuracy
    correct = predictions.filter(col("label") == col(f"pred_{batch_id}")).count()
    single_acc = correct / pred_count
    individual_accuracies.append(single_acc)
    print(f"   ğŸ“Š Single model accuracy: {single_acc*100:.2f}%")
    
    # LÆ°u predictions vÃ o HDFS
    pred_path = f"{HDFS_BASE}/predictions/rf_model_{batch_id}"
    print(f"   ğŸ’¾ Saving predictions to: {pred_path}")
    predictions.write.mode("overwrite").parquet(pred_path)
    
    model_predictions.append({
        'batch_id': batch_id,
        'pred_path': pred_path,
        'accuracy': single_acc
    })
    
    batch_elapsed = time.time() - batch_start
    print(f"   âœ… Model {batch_id} completed in {batch_elapsed:.2f}s")
    
    # AGGRESSIVE Memory cleanup
    df_batch_train.unpersist()
    df_batch_scaled.unpersist() if 'df_batch_scaled' in dir() else None
    predictions.unpersist()
    del lr_model
    del scaler_model
    del predictions
    del df_batch_train
    spark.catalog.clearCache()
    
    # Force garbage collection
    for _ in range(3):
        gc.collect()
    
    # Cooldown ngáº¯n hÆ¡n vÃ¬ LR nháº¹ hÆ¡n RF
    print("   â³ Cooldown 5s...")
    time.sleep(5)

print("\n" + "="*80)
print(f"âœ… All {len(batch_ids)} Logistic Regression models trained!")
print("="*80)

# Show individual accuracies
print("\nğŸ“Š Individual Model Accuracies:")
avg_individual = sum(individual_accuracies) / len(individual_accuracies)
for mp in model_predictions:
    print(f"   Model {mp['batch_id']}: {mp['accuracy']*100:.2f}%")
print(f"\n   ğŸ“ˆ Average: {avg_individual*100:.2f}%")

# ============================================================================
# STEP 4: Ensemble Predictions - Probability Averaging (ChÃ­nh xÃ¡c hÆ¡n Majority Voting)
# ============================================================================

print("\n" + "="*80)
print("ğŸ”® STEP 4: Ensemble Predictions (Probability Averaging)")
print("="*80)

# Load vÃ  join predictions sá»­ dá»¥ng image_id thá»±c (FIX cho bug monotonically_increasing_id)
print("ğŸ“‚ Loading prediction batch 1 as base...")
df_ensemble = spark.read.parquet(model_predictions[0]['pred_path'])
print(f"   âœ… Base loaded: {df_ensemble.count():,} rows")

# Join incrementally vá»›i cÃ¡c predictions cÃ²n láº¡i
for mp in model_predictions[1:]:
    batch_id = mp['batch_id']
    print(f"\nğŸ“‚ Loading and joining prediction batch {batch_id}...")
    
    df_pred = spark.read.parquet(mp['pred_path'])
    
    # Join báº±ng image_id (CHÃNH XÃC - khÃ´ng bá»‹ láº«n lá»™n giá»¯a cÃ¡c áº£nh)
    df_ensemble = df_ensemble.join(
        df_pred.select("image_id", f"pred_{batch_id}", f"prob_{batch_id}"),
        on="image_id",
        how="inner"
    )
    
    count = df_ensemble.count()
    print(f"   âœ… Joined: {count:,} rows")

df_ensemble = df_ensemble.repartition(10).cache()
ensemble_count = df_ensemble.count()

print(f"\nâœ… Ensemble data ready: {ensemble_count:,} samples")

# ============================================================================
# STEP 5: Calculate Ensemble Prediction vá»›i Probability Averaging
# ============================================================================

print("\n" + "="*80)
print("ğŸ¯ STEP 5: Calculating Ensemble Prediction")
print("="*80)

# TÃ­nh trung bÃ¬nh probability tá»« táº¥t cáº£ models
# ÄÃ¢y lÃ  cÃ¡ch ensemble chÃ­nh xÃ¡c hÆ¡n majority voting
prob_cols = [f"prob_{mp['batch_id']}" for mp in model_predictions]
print(f"ğŸ“Š Averaging probabilities from {len(prob_cols)} models...")

# TÃ­nh average probability cho class REAL (label=1)
avg_prob_expr = sum([col(c) for c in prob_cols]) / len(prob_cols)

df_ensemble = df_ensemble.withColumn("avg_probability", avg_prob_expr)

# Predict: náº¿u avg_prob >= 0.5 thÃ¬ predict REAL (1), ngÆ°á»£c láº¡i FAKE (0)
df_ensemble = df_ensemble.withColumn(
    "ensemble_prediction",
    when(col("avg_probability") >= 0.5, 1.0).otherwise(0.0)
)

df_ensemble = df_ensemble.cache()

print("âœ… Ensemble predictions calculated!")

# Show sample
print("\nğŸ“Š Sample predictions:")
sample_cols = ["image_id", "label"] + prob_cols[:3] + ["avg_probability", "ensemble_prediction"]
df_ensemble.select(*sample_cols).show(5, truncate=False)

# ============================================================================
# STEP 6: Evaluate Ensemble Performance trÃªn Validation Set
# ============================================================================

print("\n" + "="*80)
print("ğŸ“Š STEP 6: Evaluating Ensemble Performance (Validation Set)")
print("="*80)

# Accuracy
evaluator_acc = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="ensemble_prediction",
    metricName="accuracy"
)
ensemble_acc = evaluator_acc.evaluate(df_ensemble)

# Precision
evaluator_prec = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="ensemble_prediction",
    metricName="weightedPrecision"
)
ensemble_prec = evaluator_prec.evaluate(df_ensemble)

# Recall
evaluator_rec = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="ensemble_prediction",
    metricName="weightedRecall"
)
ensemble_rec = evaluator_rec.evaluate(df_ensemble)

# F1-Score
evaluator_f1 = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="ensemble_prediction",
    metricName="f1"
)
ensemble_f1 = evaluator_f1.evaluate(df_ensemble)

# TÃ­nh Confusion Matrix metrics thá»§ cÃ´ng
tp = df_ensemble.filter((col("label") == 1) & (col("ensemble_prediction") == 1)).count()
tn = df_ensemble.filter((col("label") == 0) & (col("ensemble_prediction") == 0)).count()
fp = df_ensemble.filter((col("label") == 0) & (col("ensemble_prediction") == 1)).count()
fn = df_ensemble.filter((col("label") == 1) & (col("ensemble_prediction") == 0)).count()

print("\nğŸ“Š Confusion Matrix:")
print(f"   TP (REAL predicted as REAL): {tp:,}")
print(f"   TN (FAKE predicted as FAKE): {tn:,}")
print(f"   FP (FAKE predicted as REAL): {fp:,}")
print(f"   FN (REAL predicted as FAKE): {fn:,}")

# ============================================================================
# STEP 7: Save Training Results to HDFS
# ============================================================================

print("\n" + "="*80)
print("ğŸ’¾ STEP 7: Saving Training Results to HDFS")
print("="*80)

# Save validation predictions (vá»›i probability Ä‘á»ƒ evaluation step cÃ³ thá»ƒ tÃ­nh AUC Ä‘Ãºng)
val_predictions_path = f"{HDFS_BASE}/results/validation_predictions"
print(f"\nğŸ“Š Saving validation predictions to: {val_predictions_path}")
df_ensemble.select(
    "image_id", "label", "ensemble_prediction", "avg_probability"
).write.mode("overwrite").parquet(val_predictions_path)
print("âœ… Validation predictions saved!")

# Save detailed predictions vá»›i táº¥t cáº£ model votes
detailed_path = f"{HDFS_BASE}/results/validation_detailed"
print(f"\nğŸ“Š Saving detailed predictions to: {detailed_path}")
df_ensemble.write.mode("overwrite").parquet(detailed_path)
print("âœ… Detailed predictions saved!")

# Save training metrics
from pyspark.sql import Row
metrics_data = [
    Row(metric="Accuracy", value=float(ensemble_acc)),
    Row(metric="Precision", value=float(ensemble_prec)),
    Row(metric="Recall", value=float(ensemble_rec)),
    Row(metric="F1_Score", value=float(ensemble_f1)),
    Row(metric="Validation_Samples", value=float(ensemble_count)),
    Row(metric="Training_Samples", value=float(train_count)),
    Row(metric="Num_Models", value=float(len(model_predictions))),
    Row(metric="MaxIter", value=float(LR_PARAMS["maxIter"])),
    Row(metric="RegParam", value=float(LR_PARAMS["regParam"])),
    Row(metric="TP", value=float(tp)),
    Row(metric="TN", value=float(tn)),
    Row(metric="FP", value=float(fp)),
    Row(metric="FN", value=float(fn)),
    Row(metric="Avg_Individual_Accuracy", value=float(avg_individual))
]
df_metrics = spark.createDataFrame(metrics_data)
metrics_path = f"{HDFS_BASE}/results/training_metrics"
print(f"\nğŸ“Š Saving training metrics to: {metrics_path}")
df_metrics.write.mode("overwrite").parquet(metrics_path)
print("âœ… Training metrics saved!")

# Save model info cho evaluation step
model_info_path = f"{HDFS_BASE}/results/model_info"
model_info_data = [
    Row(
        batch_id=mp['batch_id'],
        pred_path=mp['pred_path'],
        accuracy=float(mp['accuracy'])
    ) for mp in model_predictions
]
df_model_info = spark.createDataFrame(model_info_data)
print(f"\nğŸ“Š Saving model info to: {model_info_path}")
df_model_info.write.mode("overwrite").parquet(model_info_path)
print("âœ… Model info saved!")

print("\n" + "="*80)
print("âœ… All training results saved to HDFS!")
print("="*80)
print(f"   - Validation predictions: {val_predictions_path}")
print(f"   - Detailed predictions: {detailed_path}")
print(f"   - Training metrics: {metrics_path}")
print(f"   - Model info: {model_info_path}")

print("\n" + "="*80)
print("ğŸ“ˆ LOGISTIC REGRESSION ENSEMBLE - TRAINING RESULTS")
print("="*80)
print(f"ğŸ“Š Training samples: {train_count:,}")
print(f"ğŸ“Š Validation samples: {ensemble_count:,}")
print(f"\nğŸ¯ Validation Metrics:")
print(f"   Accuracy:  {ensemble_acc:.4f} ({ensemble_acc*100:.2f}%)")
print(f"   Precision: {ensemble_prec:.4f} ({ensemble_prec*100:.2f}%)")
print(f"   Recall:    {ensemble_rec:.4f} ({ensemble_rec*100:.2f}%)")
print(f"   F1-Score:  {ensemble_f1:.4f} ({ensemble_f1*100:.2f}%)")

print(f"\nğŸ“ˆ Ensemble Improvement:")
print(f"   Average Individual Model: {avg_individual*100:.2f}%")
print(f"   Ensemble (Prob Averaging): {ensemble_acc*100:.2f}%")
print(f"   Improvement: {(ensemble_acc - avg_individual)*100:+.2f}%")

# ============================================================================
# FINAL SUMMARY
# ============================================================================

pipeline_elapsed = time.time() - pipeline_start

print("\n" + "="*80)
print("ğŸ TRAINING PIPELINE COMPLETED")
print("="*80)
print(f"\nâ±ï¸  Total training time: {pipeline_elapsed/60:.2f} minutes")
print(f"\nğŸ“Š Models trained: {len(model_predictions)} Logistic Regression models")
print(f"ğŸ“Š Training strategy: Ensemble with Probability Averaging")
print(f"ğŸ“Š Validation split: {VALIDATION_RATIO*100:.0f}%")

print("\n" + "="*80)
print("ğŸ”§ HYPERPARAMETERS")
print("="*80)
for key, value in LR_PARAMS.items():
    print(f"   {key}: {value}")

print("\n" + "="*80)
print("ğŸ“Š FINAL VALIDATION METRICS")
print("="*80)
print(f"   Accuracy:  {ensemble_acc*100:.2f}%")
print(f"   Precision: {ensemble_prec*100:.2f}%")
print(f"   Recall:    {ensemble_rec*100:.2f}%")
print(f"   F1-Score:  {ensemble_f1*100:.2f}%")

print("\n" + "="*80)
print("ğŸ“ NOTES")
print("="*80)
print("   âœ… Test data NOT used in training phase")
print("   âœ… Use evaluation step to run final test with test data")
print("   âœ… Probability saved for proper AUC calculation in evaluation")
print("   âœ… Image IDs used for accurate ensemble join")

print("\n" + "="*80)
print("ğŸš€ Next Step: Run evaluation script with test data")
print("="*80)

# Cleanup
df_train.unpersist()
df_val.unpersist()
df_ensemble.unpersist()
spark.catalog.clearCache()

spark.stop()
print("\nâœ… Spark session stopped. Training complete!")
