# Deepfake Detection Pipeline - Big Data Project

## ğŸ“Œ Tá»•ng quan

Pipeline Big Data phÃ¢n tÃ¡n Ä‘á»ƒ phÃ¡t hiá»‡n áº£nh Deepfake sá»­ dá»¥ng:
- **Storage**: HDFS (Hadoop Distributed File System)
- **Processing**: Apache Spark (Distributed Computing)
- **ML**: Spark MLlib + Transfer Learning (ResNet50)

## ğŸ—ï¸ Kiáº¿n trÃºc Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Data Ingestion (âœ… COMPLETED)                      â”‚
â”‚  - 120,000 áº£nh JPG uploaded lÃªn HDFS                        â”‚
â”‚  - Cáº¥u trÃºc: /user/data/raw/{train,test}/{REAL,FAKE}/      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Feature Extraction (DISTRIBUTED)                   â”‚
â”‚  - Load áº£nh tá»« HDFS báº±ng binaryFiles()                      â”‚
â”‚  - Extract features báº±ng ResNet50 (UDF trÃªn Workers)        â”‚
â”‚  - Output: 2048-dim vectors                                  â”‚
â”‚  - LÆ°u Parquet vÃ o /user/data/features/                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: ML Training (Spark MLlib)                          â”‚
â”‚  - Logistic Regression                                       â”‚
â”‚  - Random Forest (100 trees)                                 â”‚
â”‚  - LÆ°u models + predictions vÃ o HDFS                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Evaluation & Business Insight                      â”‚
â”‚  - Metrics: Accuracy, Precision, Recall, F1, AUC-ROC        â”‚
â”‚  - Confusion Matrix                                          â”‚
â”‚  - LÆ°u reports vÃ o HDFS                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‚ Cáº¥u trÃºc Dá»¯ liá»‡u trÃªn HDFS

```
hdfs://namenode:8020/
â”œâ”€â”€ user/data/
â”‚   â”œâ”€â”€ raw/                          # âœ… COMPLETED - 120,000 images
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”‚   â”œâ”€â”€ REAL/  (50,000 .jpg)
â”‚   â”‚   â”‚   â””â”€â”€ FAKE/  (50,000 .jpg)
â”‚   â”‚   â””â”€â”€ test/
â”‚   â”‚       â”œâ”€â”€ REAL/  (10,000 .jpg)
â”‚   â”‚       â””â”€â”€ FAKE/  (10,000 .jpg)
â”‚   â”‚
â”‚   â”œâ”€â”€ features/                     # Output cá»§a Step 2
â”‚   â”‚   â”œâ”€â”€ train/ (Parquet)
â”‚   â”‚   â””â”€â”€ test/  (Parquet)
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                       # Trained models
â”‚   â”‚   â”œâ”€â”€ logistic_regression/
â”‚   â”‚   â””â”€â”€ random_forest/
â”‚   â”‚
â”‚   â””â”€â”€ results/                      # Predictions & Metrics
â”‚       â”œâ”€â”€ lr_predictions/
â”‚       â”œâ”€â”€ rf_predictions/
â”‚       â””â”€â”€ metrics_summary/
â”‚
â””â”€â”€ spark-logs/                       # Event logs cho History Server
```

## ğŸš€ CÃ¡ch cháº¡y Pipeline

### BÆ°á»›c 1: Copy code vÃ o Spark Master container

```bash
docker cp feature_extraction.py spark-master:/app/
docker cp ml_training.py spark-master:/app/
```

### BÆ°á»›c 2: Cháº¡y Feature Extraction

```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  --executor-memory 2g \
  --total-executor-cores 4 \
  /app/feature_extraction.py
```

**Thá»i gian dá»± kiáº¿n**: ~20-30 phÃºt vá»›i 120,000 áº£nh

### BÆ°á»›c 3: Cháº¡y ML Training

```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  --executor-memory 2g \
  --total-executor-cores 4 \
  /app/ml_training.py
```

**Thá»i gian dá»± kiáº¿n**: ~10-15 phÃºt

### HOáº¶C: Cháº¡y toÃ n bá»™ pipeline tá»± Ä‘á»™ng

```bash
python run_pipeline.py
```

## ğŸ“Š Xem káº¿t quáº£

### 1. Spark History Server
```
URL: http://localhost:18080
```

**Cáº§n chá»¥p mÃ n hÃ¬nh**:
- Job Overview (sá»‘ lÆ°á»£ng tasks, stages)
- Task Distribution (chá»©ng minh phÃ¢n tÃ¡n)
- Timeline (parallelism)

### 2. Xem metrics tá»« HDFS

```bash
# Xem metrics summary
docker exec namenode hdfs dfs -cat /user/data/results/metrics_summary/*.parquet

# Hoáº·c dÃ¹ng Spark shell
docker exec spark-master spark-shell

scala> val metrics = spark.read.parquet("hdfs://namenode:8020/user/data/results/metrics_summary")
scala> metrics.show()
```

### 3. Xem predictions

```bash
docker exec spark-master pyspark

>>> df = spark.read.parquet("hdfs://namenode:8020/user/data/results/lr_predictions")
>>> df.show(20)
>>> df.groupBy("label", "prediction").count().show()
```

## ğŸ¯ TuÃ¢n thá»§ yÃªu cáº§u Ä‘á»“ Ã¡n

### âœ… Checklist

- [x] **Dá»¯ liá»‡u lÃªn HDFS trÆ°á»›c**: 120,000 áº£nh Ä‘Ã£ upload vÃ o HDFS
- [x] **KhÃ´ng dÃ¹ng vÃ²ng láº·p local**: DÃ¹ng `binaryFiles()` vÃ  Spark transformations
- [x] **AI phÃ¢n tÃ¡n**: ResNet50 cháº¡y trong UDF trÃªn Spark Workers
- [x] **LÆ°u Parquet**: Features vÃ  predictions Ä‘á»u á»Ÿ Ä‘á»‹nh dáº¡ng Parquet
- [x] **Spark History Server**: Event logs ghi vÃ o `/spark-logs` trÃªn HDFS
- [x] **Spark MLlib**: DÃ¹ng LogisticRegression vÃ  RandomForestClassifier

## ğŸ“ˆ Expected Metrics

### Model Performance (dá»± kiáº¿n)

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| Logistic Regression | ~85-90% | ~85-90% | ~85-90% | ~85-90% |
| Random Forest | ~90-95% | ~90-95% | ~90-95% | ~90-95% |

**LÆ°u Ã½**: Metrics thá»±c táº¿ phá»¥ thuá»™c vÃ o:
- Cháº¥t lÆ°á»£ng features tá»« ResNet50
- Hyperparameters cá»§a models
- Distribution cá»§a train/test data

## ğŸ”§ Troubleshooting

### Issue 1: Out of Memory

**Giáº£i phÃ¡p**:
```bash
# TÄƒng executor memory
--executor-memory 4g
--driver-memory 4g
```

### Issue 2: Feature extraction quÃ¡ cháº­m

**Giáº£i phÃ¡p**:
```python
# TÄƒng sá»‘ partitions
df.repartition(200)

# Hoáº·c sample má»™t pháº§n data Ä‘á»ƒ test
train_sample = train_df.sample(fraction=0.1)
```

### Issue 3: Model khÃ´ng converge

**Giáº£i phÃ¡p**:
```python
# TÄƒng iterations
lr = LogisticRegression(maxIter=200)

# Hoáº·c Ä‘iá»u chá»‰nh learning rate
lr = LogisticRegression(regParam=0.001)
```

## ğŸ“ Business Insight Report Template

### CÃ¢u há»i 1: Model cÃ³ trÃ­ch xuáº¥t Ä‘á»§ thÃ´ng tin khÃ´ng?

**Tráº£ lá»i**:
- ResNet50 features (2048 dims) capture Ä‘Æ°á»£c:
  - Low-level: edges, textures
  - Mid-level: patterns, shapes  
  - High-level: object parts
  
- Accuracy >85% chá»©ng tá» features Ä‘á»§ discriminative
- Náº¿u <80%: cáº§n thá»­ models khÃ¡c (EfficientNet, ViT)

### CÃ¢u há»i 2: So sÃ¡nh Logistic Regression vs Random Forest?

**Expected findings**:
- LR: Nhanh hÆ¡n, Ä‘Æ¡n giáº£n hÆ¡n, interpretable
- RF: Accuracy cao hÆ¡n, handle non-linearity tá»‘t hÆ¡n
- Trade-off: Speed vs Performance

### CÃ¢u há»i 3: Scalability?

**Evidence**:
- Spark History UI: sá»‘ tasks cháº¡y song song
- Processing time: tá»· lá»‡ vá»›i sá»‘ executors
- HDFS: distributed storage â†’ handle TB-scale data

## ğŸ“¸ Screenshots cáº§n cÃ³

1. âœ… **Docker containers running** (docker ps)
2. âœ… **HDFS WebUI** (http://localhost:9870) - showing 120,000 files
3. **Spark Master UI** (http://localhost:8080) - showing workers
4. **Spark History Server** (http://localhost:18080):
   - Application list
   - Job stages
   - Task timeline
   - Executor stats
5. **Terminal output**: Metrics summary

## ğŸ“ Há»c Ä‘iá»ƒm chÃ­nh

### Big Data Concepts
- **Distributed Storage**: HDFS replicas, fault tolerance
- **Distributed Computing**: Spark DAG, lazy evaluation
- **Partitioning**: Data parallelism
- **Serialization**: Parquet columnar format

### ML Engineering
- **Transfer Learning**: Pretrained models
- **Feature Engineering**: Dimensionality reduction
- **Model Selection**: Classical ML on deep features
- **Evaluation**: Comprehensive metrics

## ğŸ“§ Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Kiá»ƒm tra logs: `docker logs spark-master`
2. Kiá»ƒm tra HDFS: `docker exec namenode hdfs dfsadmin -report`
3. Kiá»ƒm tra Spark UI: http://localhost:8080

Good luck! ğŸš€
