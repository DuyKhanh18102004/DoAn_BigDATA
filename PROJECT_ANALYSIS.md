# ğŸ“Š PHÃ‚N TÃCH CHI TIáº¾T Dá»° ÃN BIG DATA - DEEPFAKE DETECTION

## ğŸ“‹ TÃ“M Táº®T CHUNG

**TÃªn Dá»± Ãn**: Deepfake Detection using TensorFlow & Apache Spark  
**Má»¥c ÄÃ­ch**: XÃ¢y dá»±ng pipeline End-to-End Ä‘á»ƒ phÃ¡t hiá»‡n áº£nh deepfake sá»­ dá»¥ng Deep Learning + Big Data  
**CÃ´ng Nghá»‡ Stack**: Apache Spark, TensorFlow MobileNetV2, HDFS, Docker, Python  
**Kiáº¿n TrÃºc**: Data Lake (HDFS) â†’ Feature Extraction (ML) â†’ Model Training (Spark) â†’ Model Serving

---

## ğŸ—ï¸ KIáº¾N TRÃšC Tá»”NG QUAN

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LOCAL DATASET (data/)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   HDFS CLUSTER           â”‚
                â”‚ (Namenode + 2 Datanodes) â”‚
                â”‚ (/user/data/raw)         â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                â”‚                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  Upload     â”‚  â”‚  Feature    â”‚  â”‚   Training  â”‚
    â”‚  (Raw)      â”‚â”€â”€â”‚ Extraction  â”‚â”€â”€â”‚   (ML)      â”‚
    â”‚  HDFS       â”‚  â”‚ (TF Mobile  â”‚  â”‚   (LR)      â”‚
    â”‚             â”‚  â”‚  NetV2)     â”‚  â”‚   Spark     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                             â”‚                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Features        â”‚  â”‚   Model     â”‚
                    â”‚ (/user/data/    â”‚  â”‚ (/user/     â”‚
                    â”‚ features_tf)    â”‚  â”‚ models)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  Evaluation &      â”‚
                                    â”‚  Load & Predict    â”‚
                                    â”‚  (Metrics)         â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Cáº¤U TRÃšC THÆ¯ Má»¤C Dá»° ÃN

```
project/
â”œâ”€â”€ docker-compose.yml          # Äá»‹nh cáº¥u hÃ¬nh cÃ¡c container (HDFS, Spark)
â”œâ”€â”€ Dockerfile                  # Image Spark + TensorFlow
â”œâ”€â”€ PROJECT_ANALYSIS.md         # TÃ i liá»‡u nÃ y
â”‚
â”œâ”€â”€ data/                       # Local dataset (train/test, REAL/FAKE)
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ REAL/              # áº¢nh thá»±c ngÆ°á»i dÃ¹ng
â”‚   â”‚   â””â”€â”€ FAKE/              # áº¢nh deepfake
â”‚   â””â”€â”€ test/
â”‚       â”œâ”€â”€ REAL/
â”‚       â””â”€â”€ FAKE/
â”‚
â”œâ”€â”€ spark-config/              # Cáº¥u hÃ¬nh Spark cluster
â”‚   â”œâ”€â”€ spark-defaults.conf    # Spark tuning, memory, serialization
â”‚   â””â”€â”€ history-server.sh      # Event logging
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_pipeline.ps1       # PowerShell orchestration script
â”‚       (Tá»± Ä‘á»™ng cháº¡y 5 step pipeline)
â”‚
â””â”€â”€ src/                        # Source code Python
    â”œâ”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ 1_ingestion/           # Step 1: Data Upload
    â”‚   â”œâ”€â”€ upload_to_hdfs.py  # Upload local â†’ HDFS
    â”‚   â””â”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ 2_feature_extraction/  # Step 2: Feature Extraction
    â”‚   â”œâ”€â”€ extract_mobilenetv2_features.py
    â”‚   â”‚   (TensorFlow MobileNetV2 â†’ 1280-dim vectors)
    â”‚   â””â”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ 4_ml_training/         # Step 3: Model Training
    â”‚   â”œâ”€â”€ ml_training_tf_features.py
    â”‚   â”‚   (LogisticRegression on TF features)
    â”‚   â”œâ”€â”€ load_and_predict.py
    â”‚   â”‚   (Load model & predict on test data)
    â”‚   â”œâ”€â”€ model_utils.py
    â”‚   â”‚   (Helper class: ModelManager)
    â”‚   â””â”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ 5_evaluation/          # Step 4: Evaluation
    â”‚   â”œâ”€â”€ evaluate_tf_model.py
    â”‚   â”‚   (Metrics: Accuracy, Precision, Recall, F1)
    â”‚   â””â”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ config/                # Configuration
    â”‚   â”œâ”€â”€ hdfs_config.py     # HDFS paths, connection settings
    â”‚   â””â”€â”€ __init__.py
    â”‚
    â””â”€â”€ utils/                 # Utilities
        â”œâ”€â”€ logging_utils.py   # Centralized logging
        â””â”€â”€ __init__.py
```

---

## ğŸ”„ PIPELINE 5-STEP

### **Step 1: Data Ingestion (Upload to HDFS)**
**File**: `src/1_ingestion/upload_to_hdfs.py`

```
Local Dataset â†’ HDFS (/user/data/raw)
```

**Chá»©c nÄƒng**:
- âœ… Upload áº£nh tá»« local (`data/train`, `data/test`) lÃªn HDFS
- âœ… Kiá»ƒm tra file tá»“n táº¡i, folder structure
- âœ… Logging chi tiáº¿t quÃ¡ trÃ¬nh upload
- âœ… Há»— trá»£ test mode (max_files)

**Output**:
```
hdfs:///user/data/raw/train/REAL/  â†’ 1000+ áº£nh
hdfs:///user/data/raw/train/FAKE/  â†’ 1000+ áº£nh deepfake
hdfs:///user/data/raw/test/REAL/   â†’ Test set
hdfs:///user/data/raw/test/FAKE/   â†’ Test set
```

---

### **Step 2: Feature Extraction (TensorFlow MobileNetV2)**
**File**: `src/2_feature_extraction/extract_mobilenetv2_features.py`

```
Raw Images (HDFS) â†’ 1280-dim Feature Vectors â†’ HDFS (/user/data/features_tf)
```

**Chá»©c nÄƒng**:
- ğŸ§  **MobileNetV2** (pre-trained ImageNet)
  - Lightweight model (3.5M parameters)
  - Output: 1280-dimensional dense vector
  - Memory-efficient cho Spark distributed processing
  
- ğŸ“Š **Batch Processing** (50 batches)
  - Má»—i batch ~ 100 áº£nh
  - Tá»•ng ~5000 áº£nh train
  
- ğŸ’¾ **Memory Optimization**
  - Periodic garbage collection
  - Disk spilling Ä‘á»ƒ trÃ¡nh OOM
  - Clear TensorFlow session
  
- ğŸ”„ **Save to Parquet** (Columnar format)
  - HDFS path: `hdfs:///user/data/features_tf/train/REAL/batch_*`
  - Compressed, efficient storage

**Key Metrics Tracked**:
- Batch processing time
- Memory usage (process + system)
- Feature statistics (mean, std, min, max)

**Output**:
```
Parquet files with schema:
  - label: Int (0=FAKE, 1=REAL)
  - features: Vector (1280 dimensions)
  - batch_id: String
```

---

### **Step 3: ML Model Training**
**File**: `src/4_ml_training/ml_training_tf_features.py`

```
Features (1280-dim) + Labels â†’ Logistic Regression Model â†’ HDFS Model
```

**Chá»©c nÄƒng**:
- ğŸ“ˆ **Logistic Regression** (Binary Classification)
  - **Hyperparameters (Tuned)**:
    - `maxIter`: 300 (iterations)
    - `regParam`: 0.001 (L2 regularization)
    - `elasticNetParam`: 0.0 (pure L2, no L1)
    - `tol`: 1e-5 (tolerance)
    - `threshold`: 0.5 (decision boundary)
  
- ğŸ“š **Data Split**:
  - Training: 80% (~4000 samples)
  - Validation: 20% (~1000 samples)
  - Test: 10 batches (separate test set)

- ğŸ¯ **Evaluation Metrics**:
  - **Validation**: Accuracy, Precision, Recall, F1
  - **Test**: Same metrics for final assessment
  - **Confusion Matrix**: TP, TN, FP, FN analysis

- ğŸ’¾ **Model Persistence**:
  - **Save to HDFS**: `hdfs:///user/models/logistic_regression_tf`
  - **Format**: Spark MLlib SerializableModel
  - **Size**: ~1KB (small model)

- ğŸ“Š **Results saved**:
  - `hdfs:///user/data/results_tf/metrics_tuned` (Parquet)
  - `hdfs:///user/data/results_tf/test_predictions_tuned` (Parquet)

**Output Metrics Example**:
```
Validation Accuracy: 92.5%
Validation F1-Score: 91.8%
Test Accuracy: 91.2%
Test F1-Score: 90.5%
```

---

### **Step 4: Load Model & Predict (NEW)**
**Files**: 
- `src/4_ml_training/load_and_predict.py` (Script)
- `src/4_ml_training/model_utils.py` (Helper Class)

```
Saved Model (HDFS) â†’ Load & Predict on Test Data â†’ Predictions
```

**Chá»©c nÄƒng**:
- ğŸ”“ **Load Pre-trained Model**
  - Load tá»« `hdfs:///user/models/logistic_regression_tf`
  - Display model metadata (coefficients dimension, threshold, etc.)
  
- ğŸ”® **Make Predictions**
  - Transform test features â†’ predictions + probabilities
  - Output: prediction (0/1) + probability (0.0-1.0)
  
- ğŸ“ˆ **Evaluate Predictions**
  - Calculate metrics tá»« predictions
  - Confusion Matrix
  - Per-class performance
  
- ğŸ“‹ **Utility Class** (model_utils.py):
  ```python
  class ModelManager:
      @staticmethod
      def load_tf_model(model_path) â†’ LogisticRegressionModel
      @staticmethod
      def predict(model, df_features) â†’ predictions_df
      @staticmethod
      def get_model_info(model) â†’ dict
  ```

**CÃ¡ch sá»­ dá»¥ng trong code khÃ¡c**:
```python
from src.utils.model_utils import ModelManager

# Load model
model = ModelManager.load_tf_model()

# Predict
predictions = ModelManager.predict(model, df_features)

# Get info
info = ModelManager.get_model_info(model)
```

---

### **Step 5: Evaluation & Analysis**
**File**: `src/5_evaluation/evaluate_tf_model.py`

```
Test Predictions â†’ Comprehensive Evaluation â†’ Metrics Report
```

**Chá»©c nÄƒng**:
- ğŸ“Š **Load Test Predictions** tá»« parquet
- ğŸ¯ **Calculate Metrics**:
  - Accuracy, Precision, Recall, F1
  - ROC-AUC
  - Per-class metrics (weighted average)
  
- ğŸ” **Error Analysis**:
  - False Positives (FAKE predicted as REAL)
  - False Negatives (REAL predicted as FAKE)
  - Confidence score distribution
  
- ğŸ“ˆ **Visualizations** (optional):
  - Confusion matrix heatmap
  - ROC curve
  - Class distribution charts

- ğŸ’¾ **Save reports** to HDFS/local

---

## ğŸ› ï¸ CÃ”NG NGHá»† STACK CHI TIáº¾T

| Layer | CÃ´ng Nghá»‡ | PhiÃªn Báº£n | Chá»©c NÄƒng |
|-------|-----------|----------|----------|
| **Containerization** | Docker | Latest | Isolate, reproducibility |
| **Data Storage** | HDFS | Hadoop 3.2.1 | Distributed file system |
| **Data Processing** | Apache Spark | 3.3.0 | Distributed computing |
| **ML Framework** | TensorFlow | 2.11.0 | Deep Learning (MobileNetV2) |
| **Image Processing** | Pillow | 9.5.0 | Image loading, resizing |
| **Linear Algebra** | NumPy | 1.23.5 | Feature vectors |
| **ML Algorithms** | Spark MLlib | 3.3.0 | Logistic Regression |
| **Serialization** | Kryo | Built-in | Fast object serialization |
| **Orchestration** | PowerShell | 5.x | Script automation |

---

## ğŸ’» INFRASTRUCTURE SETUP

### **Docker Compose Services**

| Service | Image | CPU/Memory | Ports | Chá»©c NÄƒng |
|---------|-------|-----------|-------|----------|
| **namenode** | hadoop:3.2.1 | 1 CPU, 2GB | 9870 | HDFS Name node, metadata |
| **datanode-1** | hadoop:3.2.1 | 1 CPU, 2GB | N/A | HDFS Data node 1, storage |
| **datanode-2** | hadoop:3.2.1 | 1 CPU, 2GB | N/A | HDFS Data node 2, storage |
| **spark-master** | spark-py:3.3.0 | 4 CPU, 8GB | 8080, 7077 | Spark cluster master |
| **spark-worker-1** | spark-py:3.3.0 | 2 CPU, 6GB | 8081 | Spark worker node 1 |
| **spark-worker-2** | spark-py:3.3.0 | 2 CPU, 6GB | 8082 | Spark worker node 2 |

**Total Resources**: 12 CPU, 28GB RAM

### **Custom Dockerfile**
```dockerfile
FROM apache/spark-py:v3.3.0

# System dependencies
- python3-pip
- libjpeg-dev (Image processing)
- zlib1g-dev (Compression)
- libpng-dev (PNG images)

# Python packages
- tensorflow==2.11.0 (Deep Learning)
- Pillow==9.5.0 (Image operations)
- numpy==1.23.5 (Numerical computing)
- keras==2.11.0 (Neural network API)
- h5py==3.8.0 (HDF5 support)
```

---

## âš™ï¸ SPARK CONFIGURATION (spark-defaults.conf)

**Memory Management** (Tuned cho GPU-like performance):
```properties
spark.memory.fraction=0.6           # 60% for execution
spark.memory.offHeap.enabled=true   # Off-heap memory
spark.memory.offHeap.size=2g        # 2GB extra memory

spark.driver.memory=4g              # Driver node
spark.executor.memory=4g            # Worker nodes
spark.driver.maxResultSize=2g
```

**Serialization** (Performance):
```properties
spark.serializer=KryoSerializer     # Fast serialization
spark.kryoserializer.buffer.max=512m
```

**Shuffle Optimization**:
```properties
spark.shuffle.spill=true            # Disk spilling
spark.shuffle.file.buffer=64k
spark.reducer.maxSizeInFlight=96m
```

**Event Logging** (Monitoring):
```properties
spark.eventLog.enabled=true
spark.eventLog.dir=hdfs:///spark-logs
spark.history.fs.logDirectory=hdfs:///spark-logs
spark.history.ui.port=18080
```

---

## ğŸ“Š DATA PATHS (HDFS)

| Data Type | HDFS Path | Format | Samples |
|-----------|-----------|--------|---------|
| **Raw Images** | `/user/data/raw/{train,test}/{REAL,FAKE}/` | JPG/PNG | ~5000 train, ~1000 test |
| **TF Features** | `/user/data/features_tf/{train,test}/{REAL,FAKE}/batch_*` | Parquet | 50 train batches |
| **Model** | `/user/models/logistic_regression_tf` | MLlib format | ~1KB |
| **Metrics** | `/user/data/results_tf/metrics_tuned` | Parquet | Key-value metrics |
| **Predictions** | `/user/data/results_tf/test_predictions_tuned` | Parquet | Label + Prediction + Prob |
| **Spark Logs** | `/spark-logs/` | Event logs | Monitoring & debugging |

---

## ğŸš€ CÃCH CHáº Y PIPELINE

### **1. Cháº¡y Ä‘áº§y Ä‘á»§ (táº¥t cáº£ 5 steps)**
```powershell
cd scripts
.\run_pipeline.ps1
```

### **2. Cháº¡y tá»«ng step riÃªng**
```powershell
# Skip upload & feature extraction, chá»‰ cháº¡y training + evaluation
.\run_pipeline.ps1 -SkipUpload $true -SkipFeatureExtraction $true

# Chá»‰ training
.\run_pipeline.ps1 -SkipUpload $true -SkipFeatureExtraction $true -SkipLoad $true -SkipEvaluation $true

# Training + Load model
.\run_pipeline.ps1 -SkipUpload $true -SkipFeatureExtraction $true -SkipEvaluation $true
```

### **3. Cháº¡y tá»«ng script riÃªng láº»**
```bash
# Train
spark-submit --master local[2] --driver-memory 3g \
  src/4_ml_training/ml_training_tf_features.py

# Load & Predict
spark-submit --master local[2] --driver-memory 3g \
  src/4_ml_training/load_and_predict.py

# Evaluate
spark-submit --master local[2] --driver-memory 3g \
  src/5_evaluation/evaluate_tf_model.py
```

---

## ğŸ¯ KEY FEATURES & INNOVATIONS

### **1. Memory-Optimized Feature Extraction**
- âœ… MobileNetV2 (lightweight, 1280-dim output)
- âœ… Batch processing dengan periodic garbage collection
- âœ… Disk spilling for large datasets
- âœ… TensorFlow session cleanup

### **2. Distributed ML Training**
- âœ… Spark MLlib LogisticRegression (scalable)
- âœ… Tuned hyperparameters (maxIter=300, regParam=0.001)
- âœ… Train/Val/Test split (80/20/separate)
- âœ… Model persistence to HDFS

### **3. Model Reusability**
- âœ… Save model to HDFS (persistent storage)
- âœ… Load model in separate script (no retraining)
- âœ… ModelManager utility class (DRY principle)
- âœ… Predictions + confidence scores

### **4. Comprehensive Monitoring**
- âœ… Memory tracking (process + system)
- âœ… Spark event logging to HDFS
- âœ… Detailed logging per step
- âœ… Metrics visualization & export

### **5. Automated Orchestration**
- âœ… PowerShell pipeline script (5 steps)
- âœ… Skip parameters for flexibility
- âœ… Error handling & reporting
- âœ… Execution time tracking

---

## ğŸ“ˆ PERFORMANCE METRICS

**Expected Results** (Based on tuning):
- **Validation Accuracy**: 91-93%
- **Test Accuracy**: 90-92%
- **F1-Score**: 90-91%
- **Training Time**: 5-10 minutes (50 batches)
- **Feature Extraction**: 15-20 minutes (5000 images)
- **Model Size**: ~1KB (very compact)

**Memory Usage**:
- Driver: 3-4GB
- Each Executor: 3-4GB
- Total: ~12GB for 3-node setup

---

## ğŸ” BEST PRACTICES IMPLEMENTED

âœ… **Configuration Management**
- Centralized config (`hdfs_config.py`)
- No hardcoded paths

âœ… **Logging & Debugging**
- Structured logging (`logging_utils.py`)
- Memory monitoring
- Event log persistence

âœ… **Code Organization**
- Modular structure (1_ingestion â†’ 5_evaluation)
- Clear separation of concerns
- Reusable utilities

âœ… **Data Pipeline**
- Immutable data in HDFS
- Versioned results
- Parquet format (columnar, compressed)

âœ… **Error Handling**
- Try-catch blocks
- Graceful failures
- Detailed error messages

âœ… **Scalability**
- Spark distributed processing
- HDFS for large datasets
- Batch processing for memory efficiency

---

## ğŸ“š PROJECT DEPENDENCIES

```
Python 3.8+
â”œâ”€â”€ spark==3.3.0          (PySpark)
â”œâ”€â”€ tensorflow==2.11.0
â”‚   â”œâ”€â”€ keras==2.11.0
â”‚   â””â”€â”€ h5py==3.8.0
â”œâ”€â”€ pillow==9.5.0
â”œâ”€â”€ numpy==1.23.5
â””â”€â”€ logging (standard library)
```

---

## ğŸ“ LEARNING OUTCOMES

Dá»± Ã¡n nÃ y demonstrasi:

1. **Big Data Processing**: Distributed HDFS storage, Spark processing
2. **Deep Learning**: TensorFlow feature extraction, pre-trained models
3. **ML Engineering**: Model training, evaluation, persistence
4. **Data Pipeline**: ETL workflow, modular design
5. **DevOps**: Docker containerization, memory optimization
6. **Software Engineering**: Code organization, logging, error handling

---

## ğŸ“ TÃ“MO Táº®T ÄIá»‚M QUAN TRá»ŒNG

| Äiá»ƒm | MÃ´ Táº£ | TÃ¡c Äá»™ng |
|------|-------|---------|
| **End-to-End Pipeline** | Tá»« raw images â†’ predictions | Full MLOps workflow |
| **Memory Optimization** | Garbage collection, disk spilling | Xá»­ lÃ½ large datasets |
| **Model Reusability** | Save/Load mechanism | Production-ready |
| **Distributed Processing** | Spark + HDFS | Scalable to GB/TB data |
| **Comprehensive Metrics** | Accuracy, Precision, Recall, F1 | Full evaluation |
| **Automated Orchestration** | PowerShell pipeline script | Easy execution |
| **Modular Code** | Clear separation of 5 steps | Maintainability |
| **Hyperparameter Tuning** | Optimized LR params | Better accuracy |

---

**NgÃ y phÃ¢n tÃ­ch**: December 19, 2025  
**PhiÃªn báº£n**: v1.0
