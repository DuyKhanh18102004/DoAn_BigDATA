# Docker Desktop Resource Configuration Guide

## âš ï¸ Váº¥n Äá»: Docker Desktop Crash Khi Cháº¡y 120K Images

**NguyÃªn nhÃ¢n:**

- Docker Desktop memory limit (thÆ°á»ng 2-4GB default)
- 120K images = quÃ¡ nhiá»u tasks cÃ¹ng lÃºc
- ResNet50 model lá»›n (~100MB) x nhiá»u executors
- TensorFlow/PyTorch memory overhead

## âœ… GIáº¢I PHÃP 1: BATCH PROCESSING (Khuyáº¿n Nghá»‹)

### ÄÃ£ táº¡o script tá»± Ä‘á»™ng:

```bash
scripts/run_batch_extraction.bat
```

### CÃ¡ch cháº¡y:

```cmd
# Windows
cd d:\DoAn_TH_BIGDATA
scripts\run_batch_extraction.bat
```

### Æ¯u Ä‘iá»ƒm:

âœ… KhÃ´ng bá»‹ crash (má»—i batch nhá» hÆ¡n)
âœ… CÃ³ thá»ƒ theo dÃµi tá»«ng batch
âœ… Restart tá»« batch bá»‹ lá»—i (khÃ´ng máº¥t toÃ n bá»™)
âœ… Dá»… debug

### Timeline:

- Batch 1: train/REAL (50K) - 1.5-2 giá»
- Batch 2: train/FAKE (50K) - 1.5-2 giá»
- Batch 3: test/REAL (10K) - 20-30 phÃºt
- Batch 4: test/FAKE (10K) - 20-30 phÃºt
  **Total: ~4-5 giá»**

### Theo dÃµi progress:

```cmd
scripts\check_batch_progress.bat
```

## ğŸ”§ GIáº¢I PHÃP 2: TÄ‚NG DOCKER DESKTOP RESOURCES

### BÆ°á»›c 1: Má»Ÿ Docker Desktop Settings

1. Click Docker Desktop icon (system tray)
2. Settings > Resources > Advanced

### BÆ°á»›c 2: TÄƒng giá»›i háº¡n

```
CPUs: 4-6 cores (náº¿u cÃ³)
Memory: 8GB (minimum) - 12GB (recommended)
Swap: 2GB
Disk: 60GB+
```

### BÆ°á»›c 3: Apply & Restart Docker

### BÆ°á»›c 4: Cháº¡y láº¡i vá»›i reduced config

```bash
docker exec -it spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --driver-memory 3g \
  --executor-memory 3g \
  --executor-cores 2 \
  --num-executors 2 \
  /app/src/3_feature_extraction/feature_extraction.py
```

## ğŸ’¡ GIáº¢I PHÃP 3: OPTIMIZE CODE

### Giáº£m partition size trong feature_extraction.py:

```python
# Thay vÃ¬ shuffle.partitions = 200
.config("spark.sql.shuffle.partitions", "100")  # Giáº£m xuá»‘ng 100

# ThÃªm checkpoint Ä‘á»ƒ trÃ¡nh trÃ n memory
df.checkpoint()
```

### Giáº£m batch size khi load model:

```python
# Process nhá» hÆ¡n má»—i láº§n
df.repartition(50)  # Thay vÃ¬ Ä‘á»ƒ Spark tá»± Ä‘á»™ng
```

## ğŸ“Š SO SÃNH CÃC GIáº¢I PHÃP

| Giáº£i phÃ¡p            | Äá»™ á»•n Ä‘á»‹nh | Thá»i gian | Äá»™ phá»©c táº¡p |
| -------------------- | ---------- | --------- | ----------- |
| **Batch Processing** | â­â­â­â­â­ | 4-5h      | Dá»…          |
| TÄƒng Docker RAM      | â­â­â­     | 6-7h      | Trung bÃ¬nh  |
| Optimize code        | â­â­â­â­   | 5-6h      | KhÃ³         |

## ğŸ¯ KHUYáº¾N NGHá»Š

**Cho trÆ°á»ng há»£p cá»§a báº¡n:**
ğŸ‘‰ **DÃ¹ng BATCH PROCESSING** (Giáº£i phÃ¡p 1)

**LÃ½ do:**
âœ… ÄÆ¡n giáº£n nháº¥t
âœ… KhÃ´ng cáº§n config Docker
âœ… KhÃ´ng cáº§n sá»­a code
âœ… CÃ³ script sáºµn (run_batch_extraction.bat)
âœ… á»”n Ä‘á»‹nh 100%

## ğŸš€ HÆ¯á»šNG DáºªN CHáº Y BATCH

### BÆ°á»›c 1: Dá»«ng job hiá»‡n táº¡i (náº¿u Ä‘ang cháº¡y)

```cmd
# TÃ¬m application ID Ä‘ang cháº¡y
docker exec -it spark-master /opt/spark/bin/spark-submit --kill <application-id>

# Hoáº·c restart Spark
docker restart spark-master spark-worker1 spark-worker2
```

### BÆ°á»›c 2: XÃ³a features cÅ© (náº¿u Ä‘Ã£ cÃ³ partial data)

```cmd
docker exec -it namenode hdfs dfs -rm -r /user/data/features/train
docker exec -it namenode hdfs dfs -rm -r /user/data/features/test
```

### BÆ°á»›c 3: Cháº¡y batch extraction

```cmd
cd d:\DoAn_TH_BIGDATA
scripts\run_batch_extraction.bat
```

### BÆ°á»›c 4: Theo dÃµi (terminal khÃ¡c)

```cmd
# Má»Ÿ terminal má»›i
scripts\check_batch_progress.bat

# Hoáº·c xem Spark History
# Má»Ÿ browser: http://localhost:18080
```

## ğŸ“ NOTES

- Má»—i batch Ä‘á»™c láº­p, náº¿u 1 batch fail cÃ³ thá»ƒ cháº¡y láº¡i riÃªng
- Output cá»§a má»—i batch: `/user/data/features/{train|test}/{REAL|FAKE}`
- Sau khi 4 batch xong, cháº¡y ml_training.py nhÆ° bÃ¬nh thÆ°á»ng
- Features Ä‘Æ°á»£c merge tá»± Ä‘á»™ng khi load trong ml_training.py
