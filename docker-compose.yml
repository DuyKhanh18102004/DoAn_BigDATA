version: "3.8"

services:
  # ================= HDFS =================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - "9870:9870"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./data:/data:ro
    environment:
      - CLUSTER_NAME=bigdata
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    networks:
      - bigdata_network

  datanode-1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode-1
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode_1:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    networks:
      - bigdata_network

  datanode-2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode-2
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode_2:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    networks:
      - bigdata_network

  # ================= SPARK =================
  spark-master:
    image: spark-deepfake-detection:latest
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=hdfs:///spark-logs
      - SPARK_DAEMON_MEMORY=4g
    volumes:
      - spark_tmp:/tmp/spark-temp
      - ./spark-config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    depends_on:
      - namenode
    networks:
      - bigdata_network
    deploy:
      resources:
        limits:
          memory: 8g
        reservations:
          memory: 4g

  spark-worker-1:
    image: spark-deepfake-detection:latest
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=hdfs:///spark-logs
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_CORES=2
    volumes:
      - spark_worker1_tmp:/tmp/spark-temp
      - ./spark-config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    networks:
      - bigdata_network
    deploy:
      resources:
        limits:
          memory: 6g
        reservations:
          memory: 3g

  spark-worker-2:
    image: spark-deepfake-detection:latest
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=hdfs:///spark-logs
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_CORES=2
    volumes:
      - spark_worker2_tmp:/tmp/spark-temp
      - ./spark-config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    networks:
      - bigdata_network
    deploy:
      resources:
        limits:
          memory: 6g
        reservations:
          memory: 3g

  # ================= HISTORY SERVER =================
  spark-history:
    image: bde2020/spark-history-server:3.3.0-hadoop3.3
    container_name: spark-history
    ports:
      - "18080:18080"
    volumes:
      - ./spark-config/history-server.sh:/history-server.sh
    environment:
      - SPARK_HISTORY_FS_LOGDIRECTORY=hdfs://namenode:8020/spark-logs
      - SPARK_HISTORY_UI_PORT=18080
    depends_on:
      - spark-master
      - namenode
    networks:
      - bigdata_network


  # ================= STREAMLIT WEB APP =================
  streamlit-app:
    build:
      context: .
      dockerfile: Dockerfile.webapp
    container_name: streamlit-app
    ports:
      - "8501:8501"
    volumes:
      - ./src:/app/src
      - ./docs:/app/docs
      - ./debug_images:/app/debug_images
    environment:
      - SPARK_NAMENODE=namenode
      - SPARK_MASTER=spark://spark-master:7077
      - HDFS_NAMENODE=hdfs://namenode:8020
    depends_on:
      - spark-master
      - namenode
    networks:
      - bigdata_network
    deploy:
      resources:
        limits:
          memory: 4g
        reservations:
          memory: 2g

networks:
  bigdata_network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode_1:
  hadoop_datanode_2:
  spark_tmp:
  spark_worker1_tmp:
  spark_worker2_tmp:
